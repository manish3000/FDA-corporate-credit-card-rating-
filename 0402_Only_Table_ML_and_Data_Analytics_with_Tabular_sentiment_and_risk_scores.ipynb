{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lwi813x6oLbt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('credit_ratings_multimodal.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P-ps7vEorHT",
        "outputId": "8e355eba-51d1-49d7-9409-c6a62d75257b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2029 entries, 0 to 2028\n",
            "Data columns (total 46 columns):\n",
            " #   Column                              Non-Null Count  Dtype  \n",
            "---  ------                              --------------  -----  \n",
            " 0   Name                                2029 non-null   object \n",
            " 1   Ticker                              2029 non-null   object \n",
            " 2   Rating Agency Name                  2029 non-null   object \n",
            " 3   Sector                              2029 non-null   object \n",
            " 4   currentRatio                        2029 non-null   float64\n",
            " 5   quickRatio                          2029 non-null   float64\n",
            " 6   cashRatio                           2029 non-null   float64\n",
            " 7   daysOfSalesOutstanding              2029 non-null   float64\n",
            " 8   netProfitMargin                     2029 non-null   float64\n",
            " 9   pretaxProfitMargin                  2029 non-null   float64\n",
            " 10  grossProfitMargin                   2029 non-null   float64\n",
            " 11  operatingProfitMargin               2029 non-null   float64\n",
            " 12  returnOnAssets                      2029 non-null   float64\n",
            " 13  returnOnCapitalEmployed             2029 non-null   float64\n",
            " 14  returnOnEquity                      2029 non-null   float64\n",
            " 15  assetTurnover                       2029 non-null   float64\n",
            " 16  fixedAssetTurnover                  2029 non-null   float64\n",
            " 17  debtEquityRatio                     2029 non-null   float64\n",
            " 18  debtRatio                           2029 non-null   float64\n",
            " 19  effectiveTaxRate                    2029 non-null   float64\n",
            " 20  freeCashFlowOperatingCashFlowRatio  2029 non-null   float64\n",
            " 21  freeCashFlowPerShare                2029 non-null   float64\n",
            " 22  cashPerShare                        2029 non-null   float64\n",
            " 23  companyEquityMultiplier             2029 non-null   float64\n",
            " 24  ebitPerRevenue                      2029 non-null   float64\n",
            " 25  enterpriseValueMultiple             2029 non-null   float64\n",
            " 26  operatingCashFlowPerShare           2029 non-null   float64\n",
            " 27  operatingCashFlowSalesRatio         2029 non-null   float64\n",
            " 28  payablesTurnover                    2029 non-null   float64\n",
            " 29  Rating_Merged                       2029 non-null   object \n",
            " 30  Rating_Encoded_Multiclass           2029 non-null   int64  \n",
            " 31  Rating_Encoded_Binary               2029 non-null   int64  \n",
            " 32  rating_date                         2029 non-null   object \n",
            " 33  year_qtr                            2029 non-null   object \n",
            " 34  md&a                                1639 non-null   object \n",
            " 35  nlp_positivity                      1639 non-null   float64\n",
            " 36  nlp_negativity                      1639 non-null   float64\n",
            " 37  nlp_litigiousness                   1639 non-null   float64\n",
            " 38  nlp_polarity                        1639 non-null   float64\n",
            " 39  nlp_risk                            1639 non-null   float64\n",
            " 40  nlp_readability                     1639 non-null   float64\n",
            " 41  nlp_fraud                           1639 non-null   float64\n",
            " 42  nlp_safety                          1639 non-null   float64\n",
            " 43  nlp_certainty                       1639 non-null   float64\n",
            " 44  nlp_uncertainty                     1639 non-null   float64\n",
            " 45  nlp_sentiment                       1639 non-null   float64\n",
            "dtypes: float64(36), int64(2), object(8)\n",
            "memory usage: 729.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, precision_score,\n",
        "                           recall_score, f1_score, roc_auc_score, log_loss,\n",
        "                           confusion_matrix, classification_report, top_k_accuracy_score)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('credit_ratings_multimodal.csv')\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "print(\"\\n1. Data Preprocessing...\")\n",
        "\n",
        "# Drop rating agency name as requested\n",
        "df = df.drop(columns=['Rating Agency Name'], errors='ignore')\n",
        "\n",
        "# Convert date to datetime\n",
        "df['rating_date'] = pd.to_datetime(df['rating_date'], errors='coerce')\n",
        "print(f\"Date conversion completed. Earliest date: {df['rating_date'].min()}, Latest date: {df['rating_date'].max()}\")\n",
        "\n",
        "# Create label encoders for Sector and Ticker\n",
        "sector_encoder = LabelEncoder()\n",
        "ticker_encoder = LabelEncoder()\n",
        "\n",
        "df['Sector_Encoded'] = sector_encoder.fit_transform(df['Sector'])\n",
        "df['Ticker_Encoded'] = ticker_encoder.fit_transform(df['Ticker'])\n",
        "\n",
        "print(f\"Sectors encoded: {len(sector_encoder.classes_)} unique sectors\")\n",
        "print(f\"Tickers encoded: {len(ticker_encoder.classes_)} unique tickers\")\n",
        "\n",
        "# Handle missing values in NLP features (since MD&A section might have nulls)\n",
        "# We have two options: drop rows with missing NLP or impute\n",
        "print(f\"\\nMissing values in NLP features: {df['nlp_sentiment'].isnull().sum()} rows\")\n",
        "\n",
        "# For this analysis, we'll drop rows with missing NLP features\n",
        "df_clean = df.dropna(subset=['nlp_sentiment'])\n",
        "print(f\"Data shape after dropping missing NLP: {df_clean.shape}\")\n",
        "\n",
        "# Define features for two scenarios\n",
        "# Scenario 1: Without Ticker\n",
        "features_without_ticker = [\n",
        "    # Financial ratios\n",
        "    'currentRatio', 'quickRatio', 'cashRatio', 'daysOfSalesOutstanding',\n",
        "    'netProfitMargin', 'pretaxProfitMargin', 'grossProfitMargin',\n",
        "    'operatingProfitMargin', 'returnOnAssets', 'returnOnCapitalEmployed',\n",
        "    'returnOnEquity', 'assetTurnover', 'fixedAssetTurnover', 'debtEquityRatio',\n",
        "    'debtRatio', 'effectiveTaxRate', 'freeCashFlowOperatingCashFlowRatio',\n",
        "    'freeCashFlowPerShare', 'cashPerShare', 'companyEquityMultiplier',\n",
        "    'ebitPerRevenue', 'enterpriseValueMultiple', 'operatingCashFlowPerShare',\n",
        "    'operatingCashFlowSalesRatio', 'payablesTurnover',\n",
        "\n",
        "    # Sector (encoded)\n",
        "    'Sector_Encoded',\n",
        "\n",
        "    # NLP features\n",
        "    'nlp_positivity', 'nlp_negativity', 'nlp_litigiousness', 'nlp_polarity',\n",
        "    'nlp_risk', 'nlp_readability', 'nlp_fraud', 'nlp_safety',\n",
        "    'nlp_certainty', 'nlp_uncertainty', 'nlp_sentiment'\n",
        "]\n",
        "\n",
        "# Scenario 2: With Ticker\n",
        "features_with_ticker = features_without_ticker + ['Ticker_Encoded']\n",
        "\n",
        "# Target variables\n",
        "binary_target = 'Rating_Encoded_Binary'\n",
        "multiclass_target = 'Rating_Encoded_Multiclass'\n",
        "\n",
        "print(f\"\\nFeatures without ticker: {len(features_without_ticker)}\")\n",
        "print(f\"Features with ticker: {len(features_with_ticker)}\")\n",
        "\n",
        "# Split data for both scenarios\n",
        "X_without = df_clean[features_without_ticker]\n",
        "X_with = df_clean[features_with_ticker]\n",
        "y_binary = df_clean[binary_target]\n",
        "y_multi = df_clean[multiclass_target]\n",
        "\n",
        "# Standardize features\n",
        "scaler_without = StandardScaler()\n",
        "scaler_with = StandardScaler()\n",
        "\n",
        "X_without_scaled = scaler_without.fit_transform(X_without)\n",
        "X_with_scaled = scaler_with.fit_transform(X_with)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_without_train, X_without_test, y_binary_train, y_binary_test = train_test_split(\n",
        "    X_without_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "X_with_train, X_with_test, _, _ = train_test_split(\n",
        "    X_with_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "# For multiclass\n",
        "X_without_train_multi, X_without_test_multi, y_multi_train, y_multi_test = train_test_split(\n",
        "    X_without_scaled, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "X_with_train_multi, X_with_test_multi, _, _ = train_test_split(\n",
        "    X_with_scaled, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test splits created:\")\n",
        "print(f\"Binary classification - Train: {len(y_binary_train)}, Test: {len(y_binary_test)}\")\n",
        "print(f\"Multiclass classification - Train: {len(y_multi_train)}, Test: {len(y_multi_test)}\")\n",
        "\n",
        "# Define evaluation metrics function\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, is_binary=True, model_name=\"\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='binary' if is_binary else 'weighted', zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred, average='binary' if is_binary else 'weighted', zero_division=0),\n",
        "        'F1-Score': f1_score(y_test, y_pred, average='binary' if is_binary else 'weighted', zero_division=0),\n",
        "    }\n",
        "\n",
        "    # Top-K Accuracy (for multiclass)\n",
        "    if not is_binary and y_pred_proba is not None:\n",
        "        try:\n",
        "            results['Top-K Accuracy'] = top_k_accuracy_score(y_test, y_pred_proba, k=3)\n",
        "        except:\n",
        "            results['Top-K Accuracy'] = np.nan\n",
        "\n",
        "    # ROC-AUC\n",
        "    if is_binary and y_pred_proba is not None:\n",
        "        results['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "    elif not is_binary and y_pred_proba is not None:\n",
        "        try:\n",
        "            results['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "        except:\n",
        "            results['ROC-AUC'] = np.nan\n",
        "    else:\n",
        "        results['ROC-AUC'] = np.nan\n",
        "\n",
        "    # Log Loss\n",
        "    if y_pred_proba is not None:\n",
        "        results['Log Loss'] = log_loss(y_test, y_pred_proba)\n",
        "    else:\n",
        "        results['Log Loss'] = np.nan\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Error metrics\n",
        "    if is_binary:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        results['False Positive'] = fp\n",
        "        results['False Negative'] = fn\n",
        "        results['Type I Error'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        results['Type II Error'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "        results['Misclassification Error'] = (fp + fn) / len(y_test)\n",
        "    else:\n",
        "        results['Misclassification Error'] = 1 - results['Accuracy']\n",
        "\n",
        "    return results, cm\n",
        "\n",
        "# Define models for binary classification\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BINARY CLASSIFICATION MODELS (Investment Grade vs Below Investment Grade)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize models for binary classification\n",
        "binary_models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "binary_results_without = []\n",
        "binary_results_with = []\n",
        "binary_cm_without = {}\n",
        "binary_cm_with = {}\n",
        "\n",
        "# Train and evaluate without ticker\n",
        "print(\"\\nTraining models WITHOUT Ticker feature...\")\n",
        "for name, model in binary_models.items():\n",
        "    print(f\"  Training {name}...\")\n",
        "    results, cm = evaluate_model(\n",
        "        model, X_without_train, X_without_test,\n",
        "        y_binary_train, y_binary_test,\n",
        "        is_binary=True, model_name=name\n",
        "    )\n",
        "    binary_results_without.append(results)\n",
        "    binary_cm_without[name] = cm\n",
        "\n",
        "# Train and evaluate with ticker\n",
        "print(\"\\nTraining models WITH Ticker feature...\")\n",
        "for name, model in binary_models.items():\n",
        "    print(f\"  Training {name}...\")\n",
        "    results, cm = evaluate_model(\n",
        "        model, X_with_train, X_with_test,\n",
        "        y_binary_train, y_binary_test,\n",
        "        is_binary=True, model_name=name\n",
        "    )\n",
        "    binary_results_with.append(results)\n",
        "binary_cm_with[name] = cm\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_binary_without = pd.DataFrame(binary_results_without)\n",
        "df_binary_with = pd.DataFrame(binary_results_with)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTICLASS CLASSIFICATION MODELS (6 Rating Categories)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize models for multiclass\n",
        "multiclass_models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial'),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "multi_results_without = []\n",
        "multi_results_with = []\n",
        "multi_cm_without = {}\n",
        "multi_cm_with = {}\n",
        "\n",
        "# Train and evaluate without ticker\n",
        "print(\"\\nTraining multiclass models WITHOUT Ticker feature...\")\n",
        "for name, model in multiclass_models.items():\n",
        "    print(f\"  Training {name}...\")\n",
        "    results, cm = evaluate_model(\n",
        "        model, X_without_train_multi, X_without_test_multi,\n",
        "        y_multi_train, y_multi_test,\n",
        "        is_binary=False, model_name=name\n",
        "    )\n",
        "    multi_results_without.append(results)\n",
        "    multi_cm_without[name] = cm\n",
        "\n",
        "# Train and evaluate with ticker\n",
        "print(\"\\nTraining multiclass models WITH Ticker feature...\")\n",
        "for name, model in multiclass_models.items():\n",
        "    print(f\"  Training {name}...\")\n",
        "    results, cm = evaluate_model(\n",
        "        model, X_with_train_multi, X_with_test_multi,\n",
        "        y_multi_train, y_multi_test,\n",
        "        is_binary=False, model_name=name\n",
        "    )\n",
        "    multi_results_with.append(results)\n",
        "    multi_cm_with[name] = cm\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_multi_without = pd.DataFrame(multi_results_without)\n",
        "df_multi_with = pd.DataFrame(multi_results_with)\n",
        "\n",
        "# Deep Neural Network implementation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEEP NEURAL NETWORK MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def create_dnn_model(input_dim, output_dim, is_binary=True):\n",
        "    \"\"\"Create a simple DNN model\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(output_dim, activation='sigmoid' if is_binary else 'softmax')\n",
        "    ])\n",
        "\n",
        "    if is_binary:\n",
        "        model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# DNN for binary classification\n",
        "print(\"\\nTraining DNN for binary classification...\")\n",
        "dnn_binary_without = create_dnn_model(X_without_train.shape[1], 1, is_binary=True)\n",
        "dnn_binary_with = create_dnn_model(X_with_train.shape[1], 1, is_binary=True)\n",
        "\n",
        "# Train without ticker\n",
        "dnn_binary_without.fit(X_without_train, y_binary_train,\n",
        "                      epochs=50, batch_size=32,\n",
        "                      validation_split=0.2, verbose=0)\n",
        "\n",
        "# Train with ticker\n",
        "dnn_binary_with.fit(X_with_train, y_binary_train,\n",
        "                   epochs=50, batch_size=32,\n",
        "                   validation_split=0.2, verbose=0)\n",
        "\n",
        "# Evaluate DNN binary\n",
        "y_pred_dnn_without = (dnn_binary_without.predict(X_without_test) > 0.5).astype(\"int32\")\n",
        "y_pred_dnn_with = (dnn_binary_with.predict(X_with_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Get probabilities for DNN\n",
        "y_pred_proba_without = dnn_binary_without.predict(X_without_test)\n",
        "y_pred_proba_with = dnn_binary_with.predict(X_with_test)\n",
        "\n",
        "# Calculate DNN metrics for binary\n",
        "dnn_binary_results_without = {\n",
        "    'Model': 'DNN',\n",
        "    'Accuracy': accuracy_score(y_binary_test, y_pred_dnn_without),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_binary_test, y_pred_dnn_without),\n",
        "    'Precision': precision_score(y_binary_test, y_pred_dnn_without),\n",
        "    'Recall': recall_score(y_binary_test, y_pred_dnn_without),\n",
        "    'F1-Score': f1_score(y_binary_test, y_pred_dnn_without),\n",
        "    'ROC-AUC': roc_auc_score(y_binary_test, y_pred_proba_without),\n",
        "    'Log Loss': log_loss(y_binary_test, y_pred_proba_without),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_binary_test, y_pred_dnn_without)\n",
        "}\n",
        "\n",
        "dnn_binary_results_with = {\n",
        "    'Model': 'DNN',\n",
        "    'Accuracy': accuracy_score(y_binary_test, y_pred_dnn_with),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_binary_test, y_pred_dnn_with),\n",
        "    'Precision': precision_score(y_binary_test, y_pred_dnn_with),\n",
        "    'Recall': recall_score(y_binary_test, y_pred_dnn_with),\n",
        "    'F1-Score': f1_score(y_binary_test, y_pred_dnn_with),\n",
        "    'ROC-AUC': roc_auc_score(y_binary_test, y_pred_proba_with),\n",
        "    'Log Loss': log_loss(y_binary_test, y_pred_proba_with),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_binary_test, y_pred_dnn_with)\n",
        "}\n",
        "\n",
        "# Add DNN results to DataFrames\n",
        "df_binary_without = pd.concat([df_binary_without, pd.DataFrame([dnn_binary_results_without])], ignore_index=True)\n",
        "df_binary_with = pd.concat([df_binary_with, pd.DataFrame([dnn_binary_results_with])], ignore_index=True)\n",
        "\n",
        "# DNN for multiclass\n",
        "print(\"\\nTraining DNN for multiclass classification...\")\n",
        "# --- FIX STARTS HERE ---\n",
        "# The error indicated that labels go up to 6, meaning there are 7 classes (0-6).\n",
        "# So, output_dim should be 7 instead of 6.\n",
        "dnn_multi_without = create_dnn_model(X_without_train_multi.shape[1], y_multi_train.nunique(), is_binary=False)\n",
        "dnn_multi_with = create_dnn_model(X_with_train_multi.shape[1], y_multi_train.nunique(), is_binary=False)\n",
        "# --- FIX ENDS HERE ---\n",
        "\n",
        "# Train without ticker\n",
        "dnn_multi_without.fit(X_without_train_multi, y_multi_train,\n",
        "                     epochs=50, batch_size=32,\n",
        "                     validation_split=0.2, verbose=0)\n",
        "\n",
        "# Train with ticker\n",
        "dnn_multi_with.fit(X_with_train_multi, y_multi_train,\n",
        "                  epochs=50, batch_size=32,\n",
        "                  validation_split=0.2, verbose=0)\n",
        "\n",
        "# Evaluate DNN multiclass\n",
        "y_pred_dnn_multi_without = np.argmax(dnn_multi_without.predict(X_without_test_multi), axis=1)\n",
        "y_pred_dnn_multi_with = np.argmax(dnn_multi_with.predict(X_with_test_multi), axis=1)\n",
        "y_pred_proba_multi_without = dnn_multi_without.predict(X_without_test_multi)\n",
        "y_pred_proba_multi_with = dnn_multi_with.predict(X_with_test_multi)\n",
        "\n",
        "# Calculate DNN metrics for multiclass\n",
        "dnn_multi_results_without = {\n",
        "    'Model': 'DNN',\n",
        "    'Accuracy': accuracy_score(y_multi_test, y_pred_dnn_multi_without),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_multi_test, y_pred_dnn_multi_without),\n",
        "    'Precision': precision_score(y_multi_test, y_pred_dnn_multi_without, average='weighted'),\n",
        "    'Recall': recall_score(y_multi_test, y_pred_dnn_multi_without, average='weighted'),\n",
        "    'F1-Score': f1_score(y_multi_test, y_pred_dnn_multi_without, average='weighted'),\n",
        "    'ROC-AUC': roc_auc_score(y_multi_test, y_pred_proba_multi_without, multi_class='ovr'),\n",
        "    'Log Loss': log_loss(y_multi_test, y_pred_proba_multi_without),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_multi_test, y_pred_dnn_multi_without)\n",
        "}\n",
        "\n",
        "dnn_multi_results_with = {\n",
        "    'Model': 'DNN',\n",
        "    'Accuracy': accuracy_score(y_multi_test, y_pred_dnn_multi_with),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_multi_test, y_pred_dnn_multi_with),\n",
        "    'Precision': precision_score(y_multi_test, y_pred_dnn_multi_with, average='weighted'),\n",
        "    'Recall': recall_score(y_multi_test, y_pred_dnn_multi_with, average='weighted'),\n",
        "    'F1-Score': f1_score(y_multi_test, y_pred_dnn_multi_with, average='weighted'),\n",
        "    'ROC-AUC': roc_auc_score(y_multi_test, y_pred_proba_multi_with, multi_class='ovr'),\n",
        "    'Log Loss': log_loss(y_multi_test, y_pred_proba_multi_with),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_multi_test, y_pred_dnn_multi_with)\n",
        "}\n",
        "\n",
        "# Add DNN results to DataFrames\n",
        "df_multi_without = pd.concat([df_multi_without, pd.DataFrame([dnn_multi_results_without])], ignore_index=True)\n",
        "df_multi_with = pd.concat([df_multi_with, pd.DataFrame([dnn_multi_results_with])], ignore_index=True)\n",
        "\n",
        "# Ensemble Models (Stacking)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENSEMBLE MODELS (Stacking Classifier)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create ensemble for binary classification\n",
        "print(\"\\nCreating ensemble model for binary classification...\")\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train and evaluate ensemble without ticker\n",
        "stacking_clf.fit(X_without_train, y_binary_train)\n",
        "y_pred_stack_without = stacking_clf.predict(X_without_test)\n",
        "y_pred_proba_stack_without = stacking_clf.predict_proba(X_without_test)\n",
        "\n",
        "stacking_results_without = {\n",
        "    'Model': 'Ensemble (Stacking)',\n",
        "    'Accuracy': accuracy_score(y_binary_test, y_pred_stack_without),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_binary_test, y_pred_stack_without),\n",
        "    'Precision': precision_score(y_binary_test, y_pred_stack_without),\n",
        "    'Recall': recall_score(y_binary_test, y_pred_stack_without),\n",
        "    'F1-Score': f1_score(y_binary_test, y_pred_stack_without),\n",
        "    'ROC-AUC': roc_auc_score(y_binary_test, y_pred_proba_stack_without[:, 1]),\n",
        "    'Log Loss': log_loss(y_binary_test, y_pred_proba_stack_without),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_binary_test, y_pred_stack_without)\n",
        "}\n",
        "\n",
        "# Train and evaluate ensemble with ticker\n",
        "stacking_clf.fit(X_with_train, y_binary_train)\n",
        "y_pred_stack_with = stacking_clf.predict(X_with_test)\n",
        "y_pred_proba_stack_with = stacking_clf.predict_proba(X_with_test)\n",
        "\n",
        "stacking_results_with = {\n",
        "    'Model': 'Ensemble (Stacking)',\n",
        "    'Accuracy': accuracy_score(y_binary_test, y_pred_stack_with),\n",
        "    'Balanced Accuracy': balanced_accuracy_score(y_binary_test, y_pred_stack_with),\n",
        "    'Precision': precision_score(y_binary_test, y_pred_stack_with),\n",
        "    'Recall': recall_score(y_binary_test, y_pred_stack_with),\n",
        "    'F1-Score': f1_score(y_binary_test, y_pred_stack_with),\n",
        "    'ROC-AUC': roc_auc_score(y_binary_test, y_pred_proba_stack_with[:, 1]),\n",
        "    'Log Loss': log_loss(y_binary_test, y_pred_proba_stack_with),\n",
        "    'Misclassification Error': 1 - accuracy_score(y_binary_test, y_pred_stack_with)\n",
        "}\n",
        "\n",
        "# Add ensemble results\n",
        "df_binary_without = pd.concat([df_binary_without, pd.DataFrame([stacking_results_without])], ignore_index=True)\n",
        "df_binary_with = pd.concat([df_binary_with, pd.DataFrame([stacking_results_with])], ignore_index=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BINARY CLASSIFICATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nModels WITHOUT Ticker (Sorted by Accuracy):\")\n",
        "print(df_binary_without.sort_values('Accuracy', ascending=False).to_string())\n",
        "\n",
        "print(\"\\n\\nModels WITH Ticker (Sorted by Accuracy):\")\n",
        "print(df_binary_with.sort_values('Accuracy', ascending=False).to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTICLASS CLASSIFICATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nModels WITHOUT Ticker (Sorted by Accuracy):\")\n",
        "print(df_multi_without.sort_values('Accuracy', ascending=False).to_string())\n",
        "\n",
        "print(\"\\n\\nModels WITH Ticker (Sorted by Accuracy):\")\n",
        "print(df_multi_with.sort_values('Accuracy', ascending=False).to_string())\n",
        "\n",
        "# Model Ranking based on Accuracy\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL RANKING BASED ON ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nBINARY CLASSIFICATION - WITHOUT TICKER:\")\n",
        "binary_ranking_without = df_binary_without.sort_values('Accuracy', ascending=False)[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC']]\n",
        "print(binary_ranking_without.to_string())\n",
        "\n",
        "print(\"\\n\\nBINARY CLASSIFICATION - WITH TICKER:\")\n",
        "binary_ranking_with = df_binary_with.sort_values('Accuracy', ascending=False)[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC']]\n",
        "print(binary_ranking_with.to_string())\n",
        "\n",
        "print(\"\\n\\nMULTICLASS CLASSIFICATION - WITHOUT TICKER:\")\n",
        "multi_ranking_without = df_multi_without.sort_values('Accuracy', ascending=False)[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC']]\n",
        "print(multi_ranking_without.to_string())\n",
        "\n",
        "print(\"\\n\\nMULTICLASS CLASSIFICATION - WITH TICKER:\")\n",
        "multi_ranking_with = df_multi_with.sort_values('Accuracy', ascending=False)[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC']]\n",
        "print(multi_ranking_with.to_string())\n",
        "\n",
        "# Function to plot confusion matrices\n",
        "def plot_confusion_matrices(cm_dict, title_prefix):\n",
        "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "    n_models = len(cm_dict)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (model_name, cm) in enumerate(list(cm_dict.items())[:8]):\n",
        "        ax = axes[idx]\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        ax.set_title(f'{model_name}')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "\n",
        "    plt.suptitle(f'{title_prefix} - Confusion Matrices', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices for binary classification\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Note: Uncomment to plot confusion matrices\n",
        "# print(\"\\nBinary Classification - WITHOUT Ticker:\")\n",
        "# plot_confusion_matrices(binary_cm_without, \"Binary Classification Without Ticker\")\n",
        "\n",
        "# print(\"\\nBinary Classification - WITH Ticker:\")\n",
        "# plot_confusion_matrices(binary_cm_with, \"Binary Classification With Ticker\")\n",
        "\n",
        "# print(\"\\nMulticlass Classification - WITHOUT Ticker:\")\n",
        "# plot_confusion_matrices(multi_cm_without, \"Multiclass Classification Without Ticker\")\n",
        "\n",
        "# print(\"\\nMulticlass Classification - WITH Ticker:\")\n",
        "# plot_confusion_matrices(multi_cm_with, \"Multiclass Classification With Ticker\")\n",
        "\n",
        "# Comparison: With vs Without Ticker\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON: WITH vs WITHOUT TICKER FEATURE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison DataFrame for binary\n",
        "comparison_binary = pd.merge(\n",
        "    df_binary_without[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC', 'Misclassification Error']],\n",
        "    df_binary_with[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC', 'Misclassification Error']],\n",
        "    on='Model',\n",
        "    suffixes=('_Without', '_With')\n",
        ")\n",
        "\n",
        "comparison_binary['Accuracy_Diff'] = comparison_binary['Accuracy_With'] - comparison_binary['Accuracy_Without']\n",
        "comparison_binary['F1_Diff'] = comparison_binary['F1-Score_With'] - comparison_binary['F1-Score_Without']\n",
        "comparison_binary['AUC_Diff'] = comparison_binary['ROC-AUC_With'] - comparison_binary['ROC-AUC_Without']\n",
        "\n",
        "print(\"\\nBinary Classification Comparison (With - Without Ticker):\")\n",
        "print(comparison_binary[['Model', 'Accuracy_Without', 'Accuracy_With', 'Accuracy_Diff',\n",
        "                        'F1-Score_Without', 'F1-Score_With', 'F1_Diff',\n",
        "                        'ROC-AUC_Without', 'ROC-AUC_With', 'AUC_Diff']].to_string())\n",
        "\n",
        "# Create comparison DataFrame for multiclass\n",
        "comparison_multi = pd.merge(\n",
        "    df_multi_without[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC', 'Misclassification Error']],\n",
        "    df_multi_with[['Model', 'Accuracy', 'F1-Score', 'ROC-AUC', 'Misclassification Error']],\n",
        "    on='Model',\n",
        "    suffixes=('_Without', '_With')\n",
        ")\n",
        "\n",
        "comparison_multi['Accuracy_Diff'] = comparison_multi['Accuracy_With'] - comparison_multi['Accuracy_Without']\n",
        "comparison_multi['F1_Diff'] = comparison_multi['F1-Score_With'] - comparison_multi['F1-Score_Without']\n",
        "comparison_multi['AUC_Diff'] = comparison_multi['ROC-AUC_With'] - comparison_multi['ROC-AUC_Without']\n",
        "\n",
        "print(\"\\n\\nMulticlass Classification Comparison (With - Without Ticker):\")\n",
        "print(comparison_multi[['Model', 'Accuracy_Without', 'Accuracy_With', 'Accuracy_Diff',\n",
        "                       'F1-Score_Without', 'F1-Score_With', 'F1_Diff',\n",
        "                       'ROC-AUC_Without', 'ROC-AUC_With', 'AUC_Diff']].to_string())\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nDataset Information:\")\n",
        "print(f\"- Total samples: {len(df)}\")\n",
        "print(f\"- Samples after cleaning: {len(df_clean)}\")\n",
        "print(f\"- Binary classes: Investment Grade (1) vs Below Investment Grade (0)\")\n",
        "print(f\"- Multiclass categories: {y_multi_train.nunique()} rating categories (0-{y_multi_train.max()})\") # Updated description for multiclass\n",
        "print(f\"- Features without ticker: {len(features_without_ticker)}\")\n",
        "print(f\"- Features with ticker: {len(features_with_ticker)}\")\n",
        "\n",
        "print(f\"\\nClass Distribution (Binary):\")\n",
        "print(f\"- Investment Grade (1): {sum(y_binary == 1)} samples\")\n",
        "print(f\"- Below Investment Grade (0): {sum(y_binary == 0)} samples\")\n",
        "\n",
        "print(f\"\\nClass Distribution (Multiclass):\")\n",
        "rating_counts = df_clean['Rating_Merged'].value_counts()\n",
        "for rating, count in rating_counts.items():\n",
        "    print(f\"- {rating}: {count} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST MODELS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. BINARY CLASSIFICATION:\")\n",
        "print(f\"   Best WITHOUT ticker: {binary_ranking_without.iloc[0]['Model']} (Accuracy: {binary_ranking_without.iloc[0]['Accuracy']:.4f})\")\n",
        "print(f\"   Best WITH ticker: {binary_ranking_with.iloc[0]['Model']} (Accuracy: {binary_ranking_with.iloc[0]['Accuracy']:.4f})\")\n",
        "\n",
        "print(\"\\n2. MULTICLASS CLASSIFICATION:\")\n",
        "print(f\"   Best WITHOUT ticker: {multi_ranking_without.iloc[0]['Model']} (Accuracy: {multi_ranking_without.iloc[0]['Accuracy']:.4f})\")\n",
        "print(f\"   Best WITH ticker: {multi_ranking_with.iloc[0]['Model']} (Accuracy: {multi_ranking_with.iloc[0]['Accuracy']:.4f})\")\n",
        "\n",
        "print(\"\\n3. TICKER FEATURE IMPACT:\")\n",
        "print(f\"   Binary classification: Adding ticker improved accuracy for {sum(comparison_binary['Accuracy_Diff'] > 0)} out of {len(comparison_binary)} models\")\n",
        "print(f\"   Multiclass classification: Adding ticker improved accuracy for {sum(comparison_multi['Accuracy_Diff'] > 0)} out of {len(comparison_multi)} models\")\n",
        "\n",
        "# Save results to CSV\n",
        "df_binary_without.to_csv('binary_results_without_ticker.csv', index=False)\n",
        "df_binary_with.to_csv('binary_results_with_ticker.csv', index=False)\n",
        "df_multi_without.to_csv('multiclass_results_without_ticker.csv', index=False)\n",
        "df_multi_with.to_csv('multiclass_results_with_ticker.csv', index=False)\n",
        "comparison_binary.to_csv('binary_comparison.csv', index=False)\n",
        "comparison_multi.to_csv('multiclass_comparison.csv', index=False)\n",
        "\n",
        "print(\"\\nResults saved to CSV files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIurQ5oBos21",
        "outputId": "03509f13-f613-4ddc-b90f-fc28ee68f7e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "1. Data Preprocessing...\n",
            "Date conversion completed. Earliest date: 2005-08-16 00:00:00, Latest date: 2016-12-23 00:00:00\n",
            "Sectors encoded: 12 unique sectors\n",
            "Tickers encoded: 593 unique tickers\n",
            "\n",
            "Missing values in NLP features: 390 rows\n",
            "Data shape after dropping missing NLP: (1639, 47)\n",
            "\n",
            "Features without ticker: 37\n",
            "Features with ticker: 38\n",
            "\n",
            "Train/Test splits created:\n",
            "Binary classification - Train: 1311, Test: 328\n",
            "Multiclass classification - Train: 1311, Test: 328\n",
            "\n",
            "================================================================================\n",
            "BINARY CLASSIFICATION MODELS (Investment Grade vs Below Investment Grade)\n",
            "================================================================================\n",
            "\n",
            "Training models WITHOUT Ticker feature...\n",
            "  Training Logistic Regression...\n",
            "  Training K-Nearest Neighbors...\n",
            "  Training Naive Bayes...\n",
            "  Training Decision Tree...\n",
            "  Training Random Forest...\n",
            "  Training XGBoost...\n",
            "  Training SVM...\n",
            "\n",
            "Training models WITH Ticker feature...\n",
            "  Training Logistic Regression...\n",
            "  Training K-Nearest Neighbors...\n",
            "  Training Naive Bayes...\n",
            "  Training Decision Tree...\n",
            "  Training Random Forest...\n",
            "  Training XGBoost...\n",
            "  Training SVM...\n",
            "\n",
            "================================================================================\n",
            "MULTICLASS CLASSIFICATION MODELS (6 Rating Categories)\n",
            "================================================================================\n",
            "\n",
            "Training multiclass models WITHOUT Ticker feature...\n",
            "  Training Logistic Regression...\n",
            "  Training K-Nearest Neighbors...\n",
            "  Training Naive Bayes...\n",
            "  Training Decision Tree...\n",
            "  Training Random Forest...\n",
            "  Training XGBoost...\n",
            "  Training SVM...\n",
            "\n",
            "Training multiclass models WITH Ticker feature...\n",
            "  Training Logistic Regression...\n",
            "  Training K-Nearest Neighbors...\n",
            "  Training Naive Bayes...\n",
            "  Training Decision Tree...\n",
            "  Training Random Forest...\n",
            "  Training XGBoost...\n",
            "  Training SVM...\n",
            "\n",
            "================================================================================\n",
            "DEEP NEURAL NETWORK MODELS\n",
            "================================================================================\n",
            "\n",
            "Training DNN for binary classification...\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\n",
            "Training DNN for multiclass classification...\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\n",
            "================================================================================\n",
            "ENSEMBLE MODELS (Stacking Classifier)\n",
            "================================================================================\n",
            "\n",
            "Creating ensemble model for binary classification...\n",
            "\n",
            "================================================================================\n",
            "BINARY CLASSIFICATION RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Models WITHOUT Ticker (Sorted by Accuracy):\n",
            "                 Model  Accuracy  Balanced Accuracy  Precision    Recall  F1-Score   ROC-AUC  Log Loss  False Positive  False Negative  Type I Error  Type II Error  Misclassification Error\n",
            "5              XGBoost  0.826220           0.816249   0.845771  0.867347  0.856423  0.900819  0.454235            31.0            26.0      0.234848       0.132653                 0.173780\n",
            "4        Random Forest  0.823171           0.811224   0.838235  0.872449  0.855000  0.889726  0.427401            33.0            25.0      0.250000       0.127551                 0.176829\n",
            "8  Ensemble (Stacking)  0.817073           0.808596   0.843434  0.852041  0.847716  0.898346  0.403563             NaN             NaN           NaN            NaN                 0.182927\n",
            "0  Logistic Regression  0.759146           0.723021   0.744770  0.908163  0.818391  0.782390  0.534255            61.0            18.0      0.462121       0.091837                 0.240854\n",
            "3        Decision Tree  0.753049           0.733998   0.772512  0.831633  0.800983  0.733998  8.901024            48.0            33.0      0.363636       0.168367                 0.246951\n",
            "6                  SVM  0.750000           0.706710   0.728000  0.928571  0.816143  0.795397  0.520018            68.0            14.0      0.515152       0.071429                 0.250000\n",
            "1  K-Nearest Neighbors  0.743902           0.723871   0.764151  0.826531  0.794118  0.821467  0.978069            50.0            34.0      0.378788       0.173469                 0.256098\n",
            "7                  DNN  0.740854           0.737400   0.800000  0.755102  0.776903  0.822955  0.581541             NaN             NaN           NaN            NaN                 0.259146\n",
            "2          Naive Bayes  0.673780           0.613250   0.663004  0.923469  0.771855  0.683751  2.440362            92.0            15.0      0.696970       0.076531                 0.326220\n",
            "\n",
            "\n",
            "Models WITH Ticker (Sorted by Accuracy):\n",
            "                 Model  Accuracy  Balanced Accuracy  Precision    Recall  F1-Score   ROC-AUC  Log Loss  False Positive  False Negative  Type I Error  Type II Error  Misclassification Error\n",
            "5              XGBoost  0.835366           0.826376   0.855000  0.872449  0.863636  0.904182  0.447387            29.0            25.0      0.219697       0.127551                 0.164634\n",
            "8  Ensemble (Stacking)  0.832317           0.825062   0.857868  0.862245  0.860051  0.908202  0.390378             NaN             NaN           NaN            NaN                 0.167683\n",
            "4        Random Forest  0.798780           0.785869   0.818627  0.852041  0.835000  0.896123  0.418567            37.0            29.0      0.280303       0.147959                 0.201220\n",
            "0  Logistic Regression  0.756098           0.720470   0.743697  0.903061  0.815668  0.782583  0.532564            61.0            19.0      0.462121       0.096939                 0.243902\n",
            "7                  DNN  0.756098           0.737786   0.776190  0.831633  0.802956  0.818491  0.569197             NaN             NaN           NaN            NaN                 0.243902\n",
            "6                  SVM  0.746951           0.704159   0.726908  0.923469  0.813483  0.800634  0.517174            68.0            15.0      0.515152       0.076531                 0.253049\n",
            "1  K-Nearest Neighbors  0.740854           0.716373   0.753425  0.841837  0.795181  0.834783  0.860770            54.0            31.0      0.409091       0.158163                 0.259146\n",
            "3        Decision Tree  0.740854           0.727505   0.776119  0.795918  0.785894  0.727505  9.340581            45.0            40.0      0.340909       0.204082                 0.259146\n",
            "2          Naive Bayes  0.673780           0.613250   0.663004  0.923469  0.771855  0.683944  2.438044            92.0            15.0      0.696970       0.076531                 0.326220\n",
            "\n",
            "================================================================================\n",
            "MULTICLASS CLASSIFICATION RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Models WITHOUT Ticker (Sorted by Accuracy):\n",
            "                 Model  Accuracy  Balanced Accuracy  Precision    Recall  F1-Score  Top-K Accuracy   ROC-AUC   Log Loss  Misclassification Error\n",
            "5              XGBoost  0.591463           0.379255   0.601025  0.591463  0.581287        0.926829  0.864061   1.285560                 0.408537\n",
            "4        Random Forest  0.582317           0.350403   0.588654  0.582317  0.569203        0.914634  0.811487   1.249026                 0.417683\n",
            "0  Logistic Regression  0.432927           0.232665   0.433999  0.432927  0.394136        0.862805  0.759377   1.431625                 0.567073\n",
            "7                  DNN  0.429878           0.256197   0.439003  0.429878  0.417514             NaN  0.796384   1.414452                 0.570122\n",
            "1  K-Nearest Neighbors  0.426829           0.279092   0.428568  0.426829  0.420936        0.801829  0.676500   6.407404                 0.573171\n",
            "3        Decision Tree  0.390244           0.249746   0.387510  0.390244  0.386779        0.417683  0.574641  21.977837                 0.609756\n",
            "6                  SVM  0.390244           0.178605   0.427092  0.390244  0.317937        0.893293  0.804245   1.356708                 0.609756\n",
            "2          Naive Bayes  0.103659           0.159400   0.357193  0.103659  0.093618        0.417683  0.650648  25.665337                 0.896341\n",
            "\n",
            "\n",
            "Models WITH Ticker (Sorted by Accuracy):\n",
            "                 Model  Accuracy  Balanced Accuracy  Precision    Recall  F1-Score  Top-K Accuracy   ROC-AUC   Log Loss  Misclassification Error\n",
            "5              XGBoost  0.588415           0.370840   0.587766  0.588415  0.579045        0.923780  0.837486   1.280801                 0.411585\n",
            "4        Random Forest  0.573171           0.342396   0.573371  0.573171  0.553891        0.920732  0.813965   1.241252                 0.426829\n",
            "0  Logistic Regression  0.426829           0.231582   0.430326  0.426829  0.392819        0.862805  0.758071   1.436434                 0.573171\n",
            "1  K-Nearest Neighbors  0.423780           0.255845   0.415389  0.423780  0.414309        0.810976  0.681419   6.288506                 0.576220\n",
            "7                  DNN  0.405488           0.239950   0.418026  0.405488  0.396506             NaN  0.807969   1.405021                 0.594512\n",
            "3        Decision Tree  0.405488           0.257139   0.401988  0.405488  0.402559        0.432927  0.579437  21.428391                 0.594512\n",
            "6                  SVM  0.402439           0.185098   0.439398  0.402439  0.333923        0.878049  0.803658   1.353899                 0.597561\n",
            "2          Naive Bayes  0.103659           0.159400   0.356399  0.103659  0.093470        0.417683  0.650292  25.649376                 0.896341\n",
            "\n",
            "================================================================================\n",
            "FINAL MODEL RANKING BASED ON ACCURACY\n",
            "================================================================================\n",
            "\n",
            "BINARY CLASSIFICATION - WITHOUT TICKER:\n",
            "                 Model  Accuracy  F1-Score   ROC-AUC\n",
            "5              XGBoost  0.826220  0.856423  0.900819\n",
            "4        Random Forest  0.823171  0.855000  0.889726\n",
            "8  Ensemble (Stacking)  0.817073  0.847716  0.898346\n",
            "0  Logistic Regression  0.759146  0.818391  0.782390\n",
            "3        Decision Tree  0.753049  0.800983  0.733998\n",
            "6                  SVM  0.750000  0.816143  0.795397\n",
            "1  K-Nearest Neighbors  0.743902  0.794118  0.821467\n",
            "7                  DNN  0.740854  0.776903  0.822955\n",
            "2          Naive Bayes  0.673780  0.771855  0.683751\n",
            "\n",
            "\n",
            "BINARY CLASSIFICATION - WITH TICKER:\n",
            "                 Model  Accuracy  F1-Score   ROC-AUC\n",
            "5              XGBoost  0.835366  0.863636  0.904182\n",
            "8  Ensemble (Stacking)  0.832317  0.860051  0.908202\n",
            "4        Random Forest  0.798780  0.835000  0.896123\n",
            "0  Logistic Regression  0.756098  0.815668  0.782583\n",
            "7                  DNN  0.756098  0.802956  0.818491\n",
            "6                  SVM  0.746951  0.813483  0.800634\n",
            "1  K-Nearest Neighbors  0.740854  0.795181  0.834783\n",
            "3        Decision Tree  0.740854  0.785894  0.727505\n",
            "2          Naive Bayes  0.673780  0.771855  0.683944\n",
            "\n",
            "\n",
            "MULTICLASS CLASSIFICATION - WITHOUT TICKER:\n",
            "                 Model  Accuracy  F1-Score   ROC-AUC\n",
            "5              XGBoost  0.591463  0.581287  0.864061\n",
            "4        Random Forest  0.582317  0.569203  0.811487\n",
            "0  Logistic Regression  0.432927  0.394136  0.759377\n",
            "7                  DNN  0.429878  0.417514  0.796384\n",
            "1  K-Nearest Neighbors  0.426829  0.420936  0.676500\n",
            "3        Decision Tree  0.390244  0.386779  0.574641\n",
            "6                  SVM  0.390244  0.317937  0.804245\n",
            "2          Naive Bayes  0.103659  0.093618  0.650648\n",
            "\n",
            "\n",
            "MULTICLASS CLASSIFICATION - WITH TICKER:\n",
            "                 Model  Accuracy  F1-Score   ROC-AUC\n",
            "5              XGBoost  0.588415  0.579045  0.837486\n",
            "4        Random Forest  0.573171  0.553891  0.813965\n",
            "0  Logistic Regression  0.426829  0.392819  0.758071\n",
            "1  K-Nearest Neighbors  0.423780  0.414309  0.681419\n",
            "7                  DNN  0.405488  0.396506  0.807969\n",
            "3        Decision Tree  0.405488  0.402559  0.579437\n",
            "6                  SVM  0.402439  0.333923  0.803658\n",
            "2          Naive Bayes  0.103659  0.093470  0.650292\n",
            "\n",
            "================================================================================\n",
            "CONFUSION MATRICES\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: WITH vs WITHOUT TICKER FEATURE\n",
            "================================================================================\n",
            "\n",
            "Binary Classification Comparison (With - Without Ticker):\n",
            "                 Model  Accuracy_Without  Accuracy_With  Accuracy_Diff  F1-Score_Without  F1-Score_With   F1_Diff  ROC-AUC_Without  ROC-AUC_With  AUC_Diff\n",
            "0  Logistic Regression          0.759146       0.756098      -0.003049          0.818391       0.815668 -0.002723         0.782390      0.782583  0.000193\n",
            "1  K-Nearest Neighbors          0.743902       0.740854      -0.003049          0.794118       0.795181  0.001063         0.821467      0.834783  0.013316\n",
            "2          Naive Bayes          0.673780       0.673780       0.000000          0.771855       0.771855  0.000000         0.683751      0.683944  0.000193\n",
            "3        Decision Tree          0.753049       0.740854      -0.012195          0.800983       0.785894 -0.015089         0.733998      0.727505 -0.006494\n",
            "4        Random Forest          0.823171       0.798780      -0.024390          0.855000       0.835000 -0.020000         0.889726      0.896123  0.006397\n",
            "5              XGBoost          0.826220       0.835366       0.009146          0.856423       0.863636  0.007213         0.900819      0.904182  0.003363\n",
            "6                  SVM          0.750000       0.746951      -0.003049          0.816143       0.813483 -0.002660         0.795397      0.800634  0.005237\n",
            "7                  DNN          0.740854       0.756098       0.015244          0.776903       0.802956  0.026053         0.822955      0.818491 -0.004464\n",
            "8  Ensemble (Stacking)          0.817073       0.832317       0.015244          0.847716       0.860051  0.012335         0.898346      0.908202  0.009856\n",
            "\n",
            "\n",
            "Multiclass Classification Comparison (With - Without Ticker):\n",
            "                 Model  Accuracy_Without  Accuracy_With  Accuracy_Diff  F1-Score_Without  F1-Score_With   F1_Diff  ROC-AUC_Without  ROC-AUC_With  AUC_Diff\n",
            "0  Logistic Regression          0.432927       0.426829      -0.006098          0.394136       0.392819 -0.001317         0.759377      0.758071 -0.001306\n",
            "1  K-Nearest Neighbors          0.426829       0.423780      -0.003049          0.420936       0.414309 -0.006627         0.676500      0.681419  0.004918\n",
            "2          Naive Bayes          0.103659       0.103659       0.000000          0.093618       0.093470 -0.000149         0.650648      0.650292 -0.000356\n",
            "3        Decision Tree          0.390244       0.405488       0.015244          0.386779       0.402559  0.015780         0.574641      0.579437  0.004796\n",
            "4        Random Forest          0.582317       0.573171      -0.009146          0.569203       0.553891 -0.015311         0.811487      0.813965  0.002479\n",
            "5              XGBoost          0.591463       0.588415      -0.003049          0.581287       0.579045 -0.002241         0.864061      0.837486 -0.026575\n",
            "6                  SVM          0.390244       0.402439       0.012195          0.317937       0.333923  0.015986         0.804245      0.803658 -0.000588\n",
            "7                  DNN          0.429878       0.405488      -0.024390          0.417514       0.396506 -0.021008         0.796384      0.807969  0.011585\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Dataset Information:\n",
            "- Total samples: 2029\n",
            "- Samples after cleaning: 1639\n",
            "- Binary classes: Investment Grade (1) vs Below Investment Grade (0)\n",
            "- Multiclass categories: 8 rating categories (0-7)\n",
            "- Features without ticker: 37\n",
            "- Features with ticker: 38\n",
            "\n",
            "Class Distribution (Binary):\n",
            "- Investment Grade (1): 978 samples\n",
            "- Below Investment Grade (0): 661 samples\n",
            "\n",
            "Class Distribution (Multiclass):\n",
            "- BBB: 548 samples\n",
            "- BB: 385 samples\n",
            "- A: 339 samples\n",
            "- B: 225 samples\n",
            "- AA: 84 samples\n",
            "- CCC: 46 samples\n",
            "- AA+: 7 samples\n",
            "- CCC–: 5 samples\n",
            "\n",
            "================================================================================\n",
            "BEST MODELS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "1. BINARY CLASSIFICATION:\n",
            "   Best WITHOUT ticker: XGBoost (Accuracy: 0.8262)\n",
            "   Best WITH ticker: XGBoost (Accuracy: 0.8354)\n",
            "\n",
            "2. MULTICLASS CLASSIFICATION:\n",
            "   Best WITHOUT ticker: XGBoost (Accuracy: 0.5915)\n",
            "   Best WITH ticker: XGBoost (Accuracy: 0.5884)\n",
            "\n",
            "3. TICKER FEATURE IMPACT:\n",
            "   Binary classification: Adding ticker improved accuracy for 3 out of 9 models\n",
            "   Multiclass classification: Adding ticker improved accuracy for 2 out of 8 models\n",
            "\n",
            "Results saved to CSV files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMnsXu5Vp6FT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}